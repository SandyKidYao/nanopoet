{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 模型\n",
    "我们会设计一个最基础的 GPT 风格（decoder-only）架构的模型来完成生成诗词这个主题，如果你想了解这个架构以及关于 Attention 机制的一些基本原理，我强烈推荐Karpathy的讲解视频，如果你还不了解 Transformer 或者 Attention，那么值得一看。\n",
    "- [Andrej Karpathy - GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "- [中文翻译](https://www.bilibili.com/video/BV1K4LPzLEoA/)\n",
    "- [对应的 Notebook(Google Colab)](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n",
    "- [相关文档(Github)](https://github.com/karpathy/ng-video-lecture)\n",
    "\n",
    "我们会使用一个非常小型的 GPT 风格模型，然后尝试训练它从适应诗词文本和结构开始，逐步到能够按照我们想要的作者以及形式来生成“以假乱真”的诗词。"
   ],
   "id": "e119403e9982405e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T08:19:20.165309Z",
     "start_time": "2025-10-30T08:19:18.972181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"前馈层\"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_size, 4 * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * emb_size, emb_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "2e2ce576fd047baa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T08:19:20.171643Z",
     "start_time": "2025-10-30T08:19:20.168077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ParallelMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    并行化的多头注意力\n",
    "\n",
    "    关键优化：\n",
    "    1. 一次性计算所有头的 Q、K、V\n",
    "    2. 使用 reshape + transpose 实现多头\n",
    "    3. 并行计算所有头的注意力\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, head_num, block_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert emb_size % head_num == 0, \"emb_size 必须能被 head_num 整除\"\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.head_num = head_num\n",
    "        self.head_size = emb_size // head_num\n",
    "\n",
    "        # ✅ 优化 1: 一次性计算所有头的 Q、K、V\n",
    "        # 传统方式：每个头 3 次 Linear = head_num * 3 次操作\n",
    "        # 优化方式：1 次大的 Linear = 1 次操作\n",
    "        self.qkv = nn.Linear(emb_size, 3 * emb_size, bias=False)\n",
    "\n",
    "        # 输出投影\n",
    "        self.proj = nn.Linear(emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch, Time, Channels\n",
    "\n",
    "        # ✅ 优化 2: 一次性计算 Q、K、V\n",
    "        qkv = self.qkv(x)  # (B, T, 3*C)\n",
    "        q, k, v = qkv.split(self.emb_size, dim=-1)  # 每个都是 (B, T, C)\n",
    "\n",
    "        # ✅ 优化 3: Reshape 成多头格式\n",
    "        # (B, T, C) -> (B, T, num_heads, head_size) -> (B, num_heads, T, head_size)\n",
    "        q = q.view(B, T, self.head_num, self.head_size).transpose(1, 2)  # (B, heads, T, hs)\n",
    "        k = k.view(B, T, self.head_num, self.head_size).transpose(1, 2)  # (B, heads, T, hs)\n",
    "        v = v.view(B, T, self.head_num, self.head_size).transpose(1, 2)  # (B, heads, T, hs)\n",
    "\n",
    "        # ✅ 优化 4: 并行计算所有头的注意力\n",
    "        # (B, heads, T, hs) @ (B, heads, hs, T) -> (B, heads, T, T)\n",
    "        wei = (q @ k.transpose(-2, -1)) * (self.head_size ** -0.5)\n",
    "\n",
    "        # Causal masking\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # (B, heads, T, T) @ (B, heads, T, hs) -> (B, heads, T, hs)\n",
    "        out = wei @ v\n",
    "\n",
    "        # ✅ 优化 5: Concat 所有头\n",
    "        # (B, heads, T, hs) -> (B, T, heads, hs) -> (B, T, C)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # 输出投影\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ],
   "id": "1d7238bc02085afd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T08:19:20.176631Z",
     "start_time": "2025-10-30T08:19:20.174932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Block\"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, head_num, block_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.sa = ParallelMultiHeadAttention(emb_size, head_num, block_size, dropout)\n",
    "        self.ffwd = FeedForward(emb_size, dropout)\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ],
   "id": "6e0a1ca157aacb59",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T08:19:20.183319Z",
     "start_time": "2025-10-30T08:19:20.179243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT 语言模型\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, block_size, layer_num, head_num, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_embedding = nn.Embedding(block_size, emb_size)\n",
    "\n",
    "        # ✅ 使用并行版本的 Transformer Block\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(emb_size, head_num, block_size, dropout)\n",
    "            for _ in range(layer_num)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(emb_size)\n",
    "        self.lm_head = nn.Linear(emb_size, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B * T, C)\n",
    "            targets_flat = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, stop_token_ids=None):\n",
    "        \"\"\"\n",
    "        生成文本\n",
    "\n",
    "        Args:\n",
    "            idx: 初始 token IDs (B, T)\n",
    "            max_new_tokens: 最多生成多少个 token\n",
    "            temperature: 温度参数\n",
    "            top_k: top-k 采样\n",
    "            stop_token_ids: 停止 token 序列（如 [60, 61, 62, 63, 64] 代表 '<EOP>'）\n",
    "\n",
    "        Returns:\n",
    "            生成的完整序列 (B, T+generated)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "            # 检查是否生成了停止 token 序列\n",
    "            if stop_token_ids is not None and len(stop_token_ids) > 0:\n",
    "                # 检查最后 N 个 token 是否匹配停止序列\n",
    "                if idx.shape[1] >= len(stop_token_ids):\n",
    "                    last_tokens = idx[0, -len(stop_token_ids):].tolist()\n",
    "                    if last_tokens == stop_token_ids:\n",
    "                        break\n",
    "\n",
    "        return idx"
   ],
   "id": "18ff8f80b94b9e63",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在，结合准备阶段我们获得的一些数据的统计信息，让我们来来粗略设计一个合适的模型。\n",
    "- Token 种类 11868\n",
    "- PRE 数据量 11186141\n",
    "- MID 数据量 16853134\n",
    "- SFT 数据量 1806430\n",
    "- MID 数据中最长序列 444\n",
    "- SFT 数据中最长序列 242 --> 模型的上下文窗口可以设定在 256，来覆盖所有需要生成的情况\n",
    "\n",
    "根据 Chinchilla scaling laws（最优训练tokens ≈ 20×模型参数量），这里我们以 PRE 阶段和 MID 阶段的数据总量作为参考，大概做一下下面的设计：\n",
    "1. EMB_SIZE 256\n",
    "2. BLOCK_SIZE 256\n",
    "3. HEAD_NUM 8\n",
    "4. LAYER_NUM 8"
   ],
   "id": "f0a340417725c446"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T08:29:20.563829Z",
     "start_time": "2025-10-30T08:29:19.774725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 先加载一下数据，然后加载一下 tokenizer\n",
    "from nanopoet.dataset import load_raw_data\n",
    "from nanopoet.common import CharTokenizer, BEGIN\n",
    "\n",
    "tokenizer = CharTokenizer(\n",
    "    raw_text=\"\".join([\"\".join(list(d.values())) for d in load_raw_data(\"../raw\")])\n",
    ")\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_size=256,\n",
    "    block_size=256,\n",
    "    layer_num=8,\n",
    "    head_num=8,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# 统计一下模型的参数量\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"模型参数量\", total, \"可训练参数量\", trainable)\n",
    "\n",
    "model.eval()\n",
    "# 编码一个BEGIN作为提示词，让 model 随机生成一些内容做测试\n",
    "context = torch.tensor([tokenizer.encode(BEGIN)], dtype=torch.long)\n",
    "output = model.generate(context, max_new_tokens=10)\n",
    "print(\"输出内容\",tokenizer.decode(output.tolist()[0]))"
   ],
   "id": "d1cef1c41201caf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数量： 12466268 可训练参数量 12466268\n",
      "输出内容 B笏膴披覔覇裓住兢參耄\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这样，我们的模型参数总量大约是 12M，根据 Chinchilla scaling laws 的建议，训练数据大约需要 0.25B。考虑 PRE 和 MID 的训练阶段，PRE 训练 10 个 Epoch, MID 训练 10 个，这样总的参数量大约是 0.27B，基本满足要求。",
   "id": "f5e6cf6161a90a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
