{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SFT\n",
    "在 SFT 阶段，我们不再使用全量的数据，而是会局限在我们最终的任务上——根据作者、形式生成一个“以假乱真”的诗词。并且让模型学会如何根据元数据，生成诗词。为了满足这个目标，训练过程会有如下修改：\n",
    "1. 训练的数据会聚焦在我们选定的作者和形式对应的数据，而非全量数据。\n",
    "2. 我们会随机删除一些元数据作为训练数据，来让模型学会在各种情况下生成诗词。\n",
    "3. Loss 的计算方式也会有所改变，我们只计算模型生成的诗词部分的 Loss，忽略前面的元数据以及后续的 Padding 数据。\n",
    "\n",
    "Notebook 中的训练代码是简化过的，方便快速运行和理解。"
   ],
   "id": "5b03d949027a7ab6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:11:31.480891Z",
     "start_time": "2025-10-31T08:11:29.949824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from nanopoet.common import CharTokenizer\n",
    "from nanopoet.dataset import load_raw_data, split_data\n",
    "from nanopoet.model import GPTLanguageModel\n",
    "\n",
    "# 初始化随机种子，让重复执行的结果稳定\n",
    "random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# 首先加载数据、设备信息\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "data = load_raw_data(\"../raw\")\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = CharTokenizer(\"\".join([\"\".join(list(d.values())) for d in data]))\n"
   ],
   "id": "e6bf22c6d9da8676",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:11:31.691048Z",
     "start_time": "2025-10-31T08:11:31.485535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化模型结构\n",
    "block_size = 256\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_size=256,\n",
    "    block_size=block_size,\n",
    "    layer_num=8,\n",
    "    head_num=8,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "# 加载 Pre Train 的训练结果\n",
    "pretrain_state = torch.load(\"./output/03_mid_train_model.pt\", map_location=device)\n",
    "model.load_state_dict(pretrain_state, strict=True)"
   ],
   "id": "60f990eb485dd70d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:11:32.649094Z",
     "start_time": "2025-10-31T08:11:32.291601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nanopoet.common import encode_poem, filter_poem, update_poem_author\n",
    "\n",
    "# 构建 SFT 阶段使用的数据集\n",
    "filtered_data = [d for d in data if filter_poem(d)]\n",
    "updated_data = [update_poem_author(d) for d in filtered_data]\n",
    "train, val = split_data(updated_data)\n",
    "print(\"train:\", len(train))\n",
    "print(\"val:\", len(val))\n",
    "\n",
    "# 构建一个函数，来随机抹去一些数据的元数据\n",
    "def erase_metadata(poem):\n",
    "    new_poem = {\n",
    "        \"content\": poem[\"content\"],\n",
    "    }\n",
    "    if random.random() < 0.5:\n",
    "        new_poem[\"author\"] = poem[\"author\"]\n",
    "    if random.random() < 0.5:\n",
    "        new_poem[\"title\"] = poem[\"title\"]\n",
    "    if random.random() < 0.5:\n",
    "        new_poem[\"style\"] = poem[\"style\"]\n",
    "    return new_poem\n",
    "\n",
    "\n",
    "# 随机擦除一些元数据\n",
    "train_erased = [erase_metadata(p) for p in train]\n",
    "val_erased = [erase_metadata(p) for p in val]\n",
    "\n",
    "# 使用特殊 token 将每一首诗词转化成特定的序列\n",
    "train_ps = [encode_poem(t) for t in train_erased]\n",
    "val_ps = [encode_poem(t) for t in val_erased]\n",
    "\n",
    "# 打印一些例子\n",
    "print(\"转换后：\", train_ps[0])\n",
    "\n",
    "# 将转换后的序列编码成数据\n",
    "train_data = [torch.tensor(tokenizer.encode(poem), dtype=torch.long) for poem in train_ps]\n",
    "val_data = [torch.tensor(tokenizer.encode(poem), dtype=torch.long) for poem in train_ps]\n",
    "# 提取一下训练数据里的 author 和 style，用于后续生成样本提示词\n",
    "train_authors = list(set([p[\"author\"] for p in train if \"author\" in p]))\n",
    "train_styles = list(set([p[\"style\"] for p in train if \"style\" in p]))\n",
    "\n",
    "\n",
    "# 构建一个读取 sft 训练数据的函数\n",
    "def get_batch(poems_encoded, batch_size, start_token_id, end_token_id, pad_token_id, block_size, device):\n",
    "    \"\"\"随机采样一批诗词（SFT版本，使用-1 mask）\"\"\"\n",
    "    indices = torch.randint(len(poems_encoded), (batch_size,))\n",
    "    batch = [poems_encoded[i] for i in indices]\n",
    "    # 截断过长的数据\n",
    "    batch = [p[:block_size] if len(p) > block_size else p for p in batch]\n",
    "    max_len = max(len(p) for p in batch)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for poem in batch:\n",
    "        if len(poem) > 1:\n",
    "            x = poem[:-1]  # 输入序列\n",
    "            y = poem[1:]  # 目标序列\n",
    "\n",
    "            # 创建 mask：找到 START_TOKEN 的位置\n",
    "            # 注意：在 y 中查找，因为 y 是我们要预测的目标\n",
    "            y_list = y.tolist()\n",
    "\n",
    "            # 初始化为 -1（全部不计算 loss）\n",
    "            y_masked = torch.full_like(y, -1)\n",
    "\n",
    "            # 找到 START_TOKEN 在 y 中的位置\n",
    "            try:\n",
    "                start_idx = y_list.index(start_token_id)\n",
    "                # 找到 END_TOKEN 在 y 中的位置\n",
    "                end_idx = y_list.index(end_token_id, start_idx)\n",
    "                # 从 START_TOKEN 到 END_TOKEN（包括两端）设置为真实 token\n",
    "                y_masked[start_idx:end_idx + 1] = y[start_idx:end_idx + 1]\n",
    "            except ValueError:\n",
    "                # 如果没找到 START_TOKEN 或 END_TOKEN，整个序列都不计算 loss\n",
    "                pass\n",
    "\n",
    "            # 填充\n",
    "            pad_len = max_len - 1 - len(x)\n",
    "            if pad_len > 0:\n",
    "                x = torch.cat([x, torch.full((pad_len,), pad_token_id, dtype=torch.long)])\n",
    "                y_masked = torch.cat([y_masked, torch.full((pad_len,), -1, dtype=torch.long)])\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y_masked)\n",
    "\n",
    "    return torch.stack(batch_x).to(device), torch.stack(batch_y).to(device)"
   ],
   "id": "3975abd1a668bbd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 22084\n",
      "val: 2454\n",
      "转换后： BA欧阳修aT一斛珠tC今朝祖宴。可怜明夜孤灯馆。酒醒明月空床满。翠被重重，不似香肌暖。愁肠恰似沈香篆。千回万转萦还断。梦中若得相寻见。却愿春宵，一夜如年远。c\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:12:03.013876Z",
     "start_time": "2025-10-31T08:11:33.213896Z"
    }
   },
   "cell_type": "code",
   "source": "import torch.nn.functional as F\nfrom nanopoet.common import CONTENT_START, CONTENT_END, PADDING, encode_poem_prompt\n\n# 训练配置（SFT专用）\nbatch_size = 16  # SFT数据量较小，用小batch\nlearning_rate = 3e-4  # 基础学习率\ninit_lr_frac = 0.02  # 从mid的2%开始（非常保守，参考nanochat）\n# 学习率调整参考 nanochat，从训练开始就做线性衰减\n\n# 梯度裁剪\ngrad_clip = 1.0\n\nepochs = 3  # SFT通常不需要太多轮\neval_interval = 10\neval_iters = 5\n# 初始优化器\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate * init_lr_frac)\n\n# Notebook 只训练少数步，演示为主\ntotal_steps = 50\n\npad_token_id = tokenizer.encode(PADDING)[0]\nstart_token_id = tokenizer.encode(CONTENT_START)[0]\nend_token_id = tokenizer.encode(CONTENT_END)[0]\n\nfor step in range(total_steps):\n    xb, yb = get_batch(train_data, batch_size, start_token_id, end_token_id, pad_token_id, block_size, device)\n\n    logits, _ = model(xb)\n\n    # 计算loss（ignore_index=-1 会自动忽略条件和padding部分）\n    loss = F.cross_entropy(\n        logits.view(-1, tokenizer.vocab_size),\n        yb.view(-1),\n        ignore_index=-1\n    )\n\n    # 反向传播\n    optimizer.zero_grad()\n    loss.backward()\n\n    # 梯度裁剪\n    if grad_clip > 0.0:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n\n    # 应用学习率调度\n    lr_mult = 1.0 - step / total_steps\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = (learning_rate * init_lr_frac) * lr_mult\n\n    optimizer.step()\n    # 定期评估、打印Loss、学习率变化，并生成一段测试样本\n    if step % eval_interval == 0:\n        model.eval()\n        out = {}\n        for split, data in [('train', train_data), ('val', val_data)]:\n            losses = torch.zeros(eval_iters)\n            for k in range(eval_iters):\n                X, Y = get_batch(data, batch_size, start_token_id, start_token_id, pad_token_id, block_size, device)\n                logits, _ = model(X, Y)\n                losses[k] = F.cross_entropy(\n                    logits.view(-1, tokenizer.vocab_size),\n                    Y.view(-1),\n                    ignore_index=-1\n                )\n            out[split] = losses.mean()\n        current_lr = (learning_rate * init_lr_frac) * lr_mult\n        print(\n            f\"Step {step:5d} | 训练Loss: {out['train']:.4f} | 验证Loss: {out['val']:.4f} | LR: {current_lr:.2e} (x{lr_mult:.3f})\")\n\n        # 生成样本时，从训练数据的作者和风格中随机取一个，作为生成的 prompt\n        test_author = random.choice(train_authors)\n        test_syle = random.choice(train_syles)\n        # 生成多种 prompt 来看看实际使用时不同情况的样本\n        test_prompts = [\n            encode_poem_prompt(),\n            encode_poem_prompt(author=test_author),\n            encode_poem_prompt(style=test_syle),\n            encode_poem_prompt(author=test_author, style=test_syle),\n            encode_poem_prompt(author=test_author, style=test_syle, title=\"咏柳\"),\n        ]\n        for prompt in test_prompts:\n            context = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=device)\n            # 生成时不再限制长度，而是让模型生成到结束符位置\n            generated = model.generate(context, max_new_tokens=block_size, stop_token_ids=[end_token_id])\n            sample = tokenizer.decode(generated[0].tolist())\n            print(\"生成样本：\", sample)\n        model.train()",
   "id": "b88b730a5cd8b393",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     0 | 训练Loss: 3.9004 | 验证Loss: 4.1709 | LR: 6.00e-06 (x1.000)\n",
      "生成样本： B晦代耿亡aS七韻言襯瑞二名丁題惡tC子tC破頭新，白分吉春牖。名紫寒割噉一盬足。嚼日事樓業英悄當巧經元將是舞，怪。c\n",
      "生成样本： BA陆游a杖福s迎sT地㳇和t判消t蒂廛未空物愁菡間鬭，占戈犀然幸雖洞。煙茫千，是清。直草依軍。丁，官柏真消，在簾，應看深魚幾吹江壺虔。c\n",
      "生成样本： BS沁园春s芳普aS編累務良畀經tC征韻雲自歲縉疏，趨雲人謁春對自天星燈住束昂西向還人的賢夜仙勁值如梅，德似昡賎。当勝力呼創，白，點遊生。c\n",
      "生成样本： BA陆游aS沁园春sT五不晚三題楞祕錦S一梅大望蘇律诗地兩掖河穏楊憩荒搜日喜行，蒼風風猶和毛當春高，雨說光山雲勾失別，樂。房問前復千驛梅儒云醲老，虛人佇愠重月是漢傳知人飛。c\n",
      "生成样本： BA陆游aS沁园春sT咏柳tC霄，倚士還了寒短君合，得筍是人羣同。篿吳風止齊萊日斜。多伴二，霞蘚未允公道辟談來。樹宴，色輕築苦運來子，風樣籍稀韻界，春人腸得山。弭入鶴得他芳相郢。纚。c\n",
      "Step    10 | 训练Loss: 3.2729 | 验证Loss: 3.8013 | LR: 4.80e-06 (x0.800)\n",
      "生成样本： BA觉。七aS七言律诗sT長到道t三有詠中未二tC汙林青銀，今漢休。獨記得山焄朋秋得龍飛。西疏古，卜歲發師为带千一耐成在來間，迴舊，客關間殊便功。詩。嚼，，斗到衰。室，在寂。c\n",
      "生成样本： BA陆游aS七言绝來诗sTt愴雲露丰眙落滄釋二藥庭命有十遇，龍。到萬得此過袍前，喜馳飛。c\n",
      "生成样本： BS七言绝句s藉方此立德举鳴孔沙貯禁tC清之通筠醉還北遠儘霜，去重抵宮桑降奏故又良風夕根桂清背江，好期真看邊。c\n",
      "生成样本： BA陆游aS七言绝句s惬送韵醉tC夜自詩忍郡紺憶知魄練，艷名愛漁草耿冠。不長柳城圩髯何須公符事，作江静爛聊酬相弄一吹鳥眼不走雲塞長落園愜塔，枕家野無志，久恨別趣芳老報忍日戶。c\n",
      "生成样本： BA陆游aS七言绝句sT咏柳tC名，絲處天道兵怨繫江笑妨漿親力何瀟人傅陰長雲色想寐戶義山黄南桃，快正何。c\n",
      "Step    20 | 训练Loss: 2.8307 | 验证Loss: 3.6378 | LR: 3.60e-06 (x0.600)\n",
      "生成样本： BA籍出花aS七徐成早sT州灘tC音歸南海酒低雄睡，波去何華自起落來竹，堂只憶像闊美深時空無以人律。傷讙晚向多若山人勋扫國致貺回。颖柳欹嫣歸色调涵懷津清水孤雲天知却裹幾雲以槍山須傳屏辭，可舟露，千月有史稀。去，名。c\n",
      "生成样本： BA陆游a凰丘aS其言介s洳編臺羞吳恩月團C淮冶冢駕，何平霞好。千社魂。蛩得寞鬢，江得氛消來憶從當雙素經獨點道去堤月文幽。可參减蔬樓擬問高自鈞一世前華過，當數休登来漢，敢觚。c\n",
      "生成样本： BS七言律诗s升一亦書還芙甚排可言溟伴寂教宰枉第詔吞不疾鳳霜趙無採蕭山睡快生鳳南衾梢憑百柳有拱玉擔嘶高長臣。堪鬚荔清。奈臺勉上休，種徑解匆。晴飛摻別人乐日言剩歸有飯艇陵。c\n",
      "生成样本： BA陆游aS七言律诗s綸盧酴二莫爭中晚延物兩云鵩映力居樹春江年t頂元城草人殺當花望寺與獵致廬事，何便夜消釂須柳空。马祖陵山得顧支虚燎猊仕詩二尚憐都似事水隔楊忽湖。c\n",
      "生成样本： BA陆游aS七言律诗sT咏柳t璵雲庭諸覺蘇促巷友悔尚事此下園飛，恐縣分破蓬。鄉十兄丘簡，，妙秋衰。鈴别。好，原獨千桃，家人雨不凌渾水中滿语逢山歡。c\n",
      "Step    30 | 训练Loss: 3.5408 | 验证Loss: 3.1569 | LR: 2.40e-06 (x0.400)\n",
      "生成样本： BA描aS檻饑隠城好sT紙南題宮罅並直憑用tC杳摘，雨徵翠登垂挹半青還生重無血如天浴能我然。已漠，分。c\n",
      "生成样本： BA杜甫aS五言晝淮竺诗辜峰t其海紆李竹，子愁未極束楚詩萑歸。使，疏深甌。早，時明。頌是古枕舊，像，同。c\n",
      "生成样本： BS沁园春sa助律彭二崎卿雪佐城菟負六tC名。讀蕚相眼，此思水濟敲耐也游。一阳參凝月，日棲修。翩馮未成他輕。山屈湖山言作，冉處摇層句眄白江。早重陽阶自情。c\n",
      "生成样本： BA杜甫aS沁园春sT李五只佩散耐轉泉劉总栊和重拚有歸無用言律便不一昔門來，，小出神葛朝夢城樓羔草來谁，中門。寄更，貢孟不亦滿滿文婆燠。歲行敧聞表生。c\n",
      "生成样本： BA杜甫aS沁园春sT咏柳tC長欄边曾禁盡为凟別何當，海一訣山中朝歌鎡亦瀉緣鴻。竹蟲羣必松苦，薄灯公時中。得青問羅，去調賡旋空，淞。c\n",
      "Step    40 | 训练Loss: 3.3219 | 验证Loss: 2.6914 | LR: 1.20e-06 (x0.200)\n",
      "生成样本： BA雨隱澈臣aS小問九言鷃五有言見tC遊小綠遠著，盤清殘千春。天烈，人尤。霜邊多開。嗟鸯來三毙，星早道海人葉輪。像，水女由非阜烟王雨揚看那昔鏡仙前有，蕃風，带插黄。c\n",
      "生成样本： BA陆游aS渔七言绝。堪七未七，試tC風携老從轉，塗少笑荷白自照南去侶，今先八珍仙。c\n",
      "生成样本： BS七言绝句sT然虞賀部簾t范素縦樹府，苔高三人正中君清。霜閑。明梨，筆被更簾健語星聞面新韻入稀外林覢別聞棠初十儿。材戱自負翠雜海丞不對僞來是幸，風書瀉舊光故少子伤緣。倫。c\n",
      "生成样本： BA陆游aS七言绝句sT遊府代蕭新礫離叢年流克桃t鵾如笑肌，日烏民全三尚向不鐵略帝來紛鱗堪试八時向飛怕今餘玄浮蒼。春樽戒庭復人絲草欲仙。三見，一材彥有葉雲人收。讲涯未知場钓吞如高。c\n",
      "生成样本： BA陆游aS七言绝句sT咏柳tC雲幾畏白名懷蹀翅樂吻，玄來灑閑一地巒且塵敲天道花㳅風人見帝竹，醉，照照残。巘何人以身許年賞署年瓜，戲。聒清平分作丞青然疏倚得須共久，寂應元雲天世在不谷萬久鴟梦笙底策，身。旋避一無智。c\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:12:28.100851Z",
     "start_time": "2025-10-31T08:12:27.688767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 把训练结果保存下来，给后续训练使用\n",
    "output_path = Path(\"./output/04_sft_model.pt\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print(f\"模型已保存到 {output_path}\")"
   ],
   "id": "81afdc8d7b5a6c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存到 output/04_sft_model.pt\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c5198f7b9751fce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
