{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Mid Train 中期训练\n",
    "Mid Train 会在 Pre Train 的基础上，进一步特化模型学到的内容，让其学习使用各种特殊 token、感知元数据（标题、形式、作者）并与诗词本体建立一些关联。与 Pre Train 阶段相比，会有如下变化：\n",
    "1. 每一首诗词通过设定好的特殊 token 进行编码，生成一个独立的序列。\n",
    "2. 每一首诗词作为一份单独的训练数据，长度的问题会通过 Padding 的方式补齐，Padding 的部分不会计算 Loss。\n",
    "3. 使用更精细的训练策略，让模型准确学到一个完整诗词序列的模式。\n",
    "\n",
    "Notebook 中的训练代码是简化过的，方便快速运行和理解。"
   ],
   "id": "ad6dbba11965463"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:48.552778Z",
     "start_time": "2025-10-31T03:46:46.560232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from nanopoet.common import CharTokenizer, PADDING\n",
    "from nanopoet.dataset import load_raw_data, split_data\n",
    "from nanopoet.model import GPTLanguageModel\n",
    "\n",
    "# 初始化随机种子，让重复执行的结果稳定\n",
    "random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# 首先加载数据、设备信息\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "data = load_raw_data(\"../raw\")\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = CharTokenizer(\"\".join([\"\".join(list(d.values())) for d in data]))\n"
   ],
   "id": "b5f7eafe6f121fa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:49.433356Z",
     "start_time": "2025-10-31T03:46:49.194302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化模型结构\n",
    "block_size = 256\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_size=256,\n",
    "    block_size=block_size,\n",
    "    layer_num=8,\n",
    "    head_num=8,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "# 加载 Pre Train 的训练结果\n",
    "pretrain_state = torch.load(\"./output/02_pre_train_model.pt\", map_location=device)\n",
    "model.load_state_dict(pretrain_state, strict=True)"
   ],
   "id": "3bb61ddf90d0eae1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:53.731912Z",
     "start_time": "2025-10-31T03:46:50.553049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nanopoet.common import encode_poem\n",
    "\n",
    "# 构建 Mid Train 阶段使用的数据集\n",
    "train, val = split_data(data)\n",
    "\n",
    "# 使用特殊 token 将每一首诗词转化成特定的序列\n",
    "train_ps = [encode_poem(t) for t in train]\n",
    "val_ps = [encode_poem(t) for t in val]\n",
    "\n",
    "# 打印对比一下转换前后\n",
    "print(\"转换前：\", train[0])\n",
    "print(\"\\n转换后：\", train_ps[0])\n",
    "\n",
    "# 将转换后的序列编码成数据\n",
    "train_data = [torch.tensor(tokenizer.encode(poem), dtype=torch.long) for poem in train_ps]\n",
    "val_data = [torch.tensor(tokenizer.encode(poem), dtype=torch.long) for poem in train_ps]\n",
    "\n",
    "\n",
    "# 制作一个数据加载函数，这里与 Pre 阶段的策略是不同的，需要注意\n",
    "def get_batch(poems_encoded, batch_size, pad_token_id, block_size, device):\n",
    "    # 随机抽取一个批次的诗词\n",
    "    indices = torch.randint(len(poems_encoded), (batch_size,))\n",
    "    batch = [poems_encoded[i] for i in indices]\n",
    "    # 由于诗词数据的长度不同，但我们训练使用的上下文长度（block_size）是固定的\n",
    "    # 因此需要对超长的进行截断、过短的使用 padding token 补齐\n",
    "    # 补齐的部分，不需要进行 Loss 计算（毕竟我们不希望模型学会输出一堆没有意义的 Padding 字符）\n",
    "    # 因此需要返回补齐的 Mask，用来遮罩 Loss\n",
    "    batch = [p[:block_size] if len(p) > block_size else p for p in batch]\n",
    "    # 获取批次中的最大长度\n",
    "    # 可能这一批次中的数据，长度全都小于 block_size，如果是这样为了节约计算，不需要都强制补齐到block_size，只要对齐一个批次的数据长度即可\n",
    "    max_len = max(len(p) for p in batch)\n",
    "    # 创建填充后的批次\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    batch_mask = []\n",
    "    for poem in batch:\n",
    "        # 输入: poem[:-1], 目标: poem[1:]\n",
    "        if len(poem) > 1:\n",
    "            x = poem[:-1]\n",
    "            y = poem[1:]\n",
    "\n",
    "            # 填充\n",
    "            pad_len = max_len - 1 - len(x)\n",
    "            if pad_len > 0:\n",
    "                x = torch.cat([x, torch.full((pad_len,), pad_token_id, dtype=torch.long)])\n",
    "                y = torch.cat([y, torch.full((pad_len,), pad_token_id, dtype=torch.long)])\n",
    "                # mask: 真实token为1，填充为0\n",
    "                mask = torch.cat(\n",
    "                    [torch.ones(len(poem) - 1, dtype=torch.long), torch.zeros(pad_len, dtype=torch.long)])\n",
    "            else:\n",
    "                mask = torch.ones(len(x), dtype=torch.long)\n",
    "\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            batch_mask.append(mask)\n",
    "    return torch.stack(batch_x).to(device), torch.stack(batch_y).to(device), torch.stack(batch_mask).to(device)\n"
   ],
   "id": "404e6c6e1c68f858",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换前： {'content': '卧虹千尺界湖光。冷浸月茫茫。当日三高何处，渔唱入凄凉。人世事，纵轩裳。梦黄梁。有谁蓑笠，一钓丝风，吹尽荷香。', 'title': '一丝风诉衷情令', 'author': '张辑', 'style': '一丝风诉衷情令'}\n",
      "\n",
      "转换后： BA张辑aS一丝风诉衷情令sT一丝风诉衷情令tC卧虹千尺界湖光。冷浸月茫茫。当日三高何处，渔唱入凄凉。人世事，纵轩裳。梦黄梁。有谁蓑笠，一钓丝风，吹尽荷香。c\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:55.808069Z",
     "start_time": "2025-10-31T03:46:54.662577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置训练配置\n",
    "batch_size = 32  # 独立诗词样本用更小的batch size\n",
    "max_steps = 100\n",
    "eval_interval = 10\n",
    "eval_iters = 5\n",
    "\n",
    "# Mid 阶段训练我们使用更精细的学习率控制策略，让学习率能够随着训练过程的逐渐加深，而减小学习率\n",
    "# 学习率调度（参考 nanochat mid-train）\n",
    "learning_rate = 3e-4\n",
    "init_lr_frac = 0.5  # Mid训练从预训练LR的50%开始（更保守）\n",
    "warmdown_start_ratio = 0.8  # 前80%保持不变，最后20%线性衰减到0\n",
    "final_lr_frac = 0.0\n",
    "\n",
    "# 梯度裁剪\n",
    "# 设定一个梯度裁剪可以防止某个 batch 导致的梯度变化大幅增加，近期模型训练因梯度爆炸导致无法收敛的情况\n",
    "grad_clip = 1.0\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate * init_lr_frac\n",
    ")\n"
   ],
   "id": "5dc190d6ecd359da",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T04:11:06.470012Z",
     "start_time": "2025-10-31T04:10:40.358981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nanopoet.common import BEGIN,PADDING\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 制作一个loss 计算函数，计算 masking 之后的 loss\n",
    "def get_loss_with_mask(logits, yb, mask):\n",
    "    # 计算loss（只计算非填充位置）\n",
    "    B, T, C = logits.shape\n",
    "    logits_flat = logits.view(B * T, C)\n",
    "    targets_flat = yb.view(B * T)\n",
    "    mask_flat = mask.view(B * T)\n",
    "    # 只计算mask=1的位置\n",
    "    if mask_flat.sum() > 0:\n",
    "        loss = F.cross_entropy(logits_flat[mask_flat == 1], targets_flat[mask_flat == 1])\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=device)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# 简化的训练循环\n",
    "pad_token_id = tokenizer.encode(PADDING)[0]\n",
    "for step in range(max_steps):\n",
    "    # 获取对齐后的数据以及填充 mask\n",
    "    xb, yb, mask = get_batch(train_data, batch_size, pad_token_id, block_size, device)\n",
    "    # 前向传播，但忽略模型直接返回的 Loss 值\n",
    "    logits, _ = model(xb, yb)\n",
    "    # 单独计算 loss\n",
    "    loss = get_loss_with_mask(logits, yb, mask)\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 梯度裁剪\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "    # 计算学习率变化\n",
    "    warmdown_start_step = int(warmdown_start_ratio * max_steps)\n",
    "\n",
    "    if step < warmdown_start_step:\n",
    "        # 第一阶段，学习率不变\n",
    "        lr_mult = 1.0\n",
    "    else:\n",
    "        # 最后阶段，学习率随着 step 增加线性衰减\n",
    "        progress = (max_steps - step) / (max_steps - warmdown_start_step)\n",
    "        lr_mult = progress * 1.0 + (1 - progress) * final_lr_frac\n",
    "\n",
    "    # 应用新的学习率\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate * lr_mult\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # 定期评估、打印Loss、学习率变化，并生成一段测试样本\n",
    "    if step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        out = {}\n",
    "        for split, data in [('train', train_data), ('val', val_data)]:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X,Y,M = get_batch(data, batch_size, pad_token_id, block_size, device)\n",
    "                logits, _ = model(X, Y)\n",
    "                losses[k] = get_loss_with_mask(logits, Y, M).item()\n",
    "            out[split] = losses.mean()\n",
    "        context = torch.tensor([tokenizer.encode(BEGIN)], dtype=torch.long, device=device)\n",
    "        generated = model.generate(context, max_new_tokens=20)\n",
    "        sample = tokenizer.decode(generated[0].tolist())\n",
    "        model.train()\n",
    "        current_lr = (learning_rate * init_lr_frac) * lr_mult\n",
    "\n",
    "        print(f\"Step {step:5d} | 训练Loss: {out['train']:.4f} | 验证Loss: {out['val']:.4f} | LR: {current_lr:.2e} (x{lr_mult:.3f})\")\n",
    "        print(\"生成样本：\", sample)\n",
    "\n"
   ],
   "id": "748be8c69ac492f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     0 | 训练Loss: 7.4114 | 验证Loss: 7.4523 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： B業鬐羹當巧經元將是舞，战。認杖福泛迎。行\n",
      "Step    10 | 训练Loss: 6.9597 | 验证Loss: 6.9636 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： B獿厩宴諡蘧罚築苦運來子，風樣其素韻界钜花\n",
      "Step    20 | 训练Loss: 6.6421 | 验证Loss: 6.6594 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： B皿歸䜝散坂柧事浇。雲誤浮路迹度絲能，此。\n",
      "Step    30 | 训练Loss: 6.3811 | 验证Loss: 6.3683 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： B侉趣恋珏報決疲醓運牛遂言，宋畫阿道兵怨虎\n",
      "Step    40 | 训练Loss: 6.1743 | 验证Loss: 6.1063 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： B密藞霜丘搔切井巾S莎節嫣交欠渺脚舍相送處\n",
      "Step    50 | 训练Loss: 6.0228 | 验证Loss: 5.9918 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： B嘶高锺臣駷a硬荔Ss據餉勉上与陽種徑t讨\n",
      "Step    60 | 训练Loss: 5.7672 | 验证Loss: 5.8326 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： BA鹜a睎a绤滿書綸s辔任五素計奉玉立泉去\n",
      "Step    70 | 训练Loss: 5.7084 | 验证Loss: 5.6721 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： B師蘼a罙蕚S韵德汗saT敲耐也游tC阳參\n",
      "Step    80 | 训练Loss: 5.6232 | 验证Loss: 5.6168 | LR: 1.50e-04 (x1.000)\n",
      "生成样本： BA次窗aS題徘临處s轿之憂三湘其當一土t\n",
      "Step    90 | 训练Loss: 5.6796 | 验证Loss: 5.7431 | LR: 7.50e-05 (x0.500)\n",
      "生成样本： B莊A荷aS七言绝侶s虻七八珍居赴S秋然虞\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T04:15:30.234509Z",
     "start_time": "2025-10-31T04:15:29.696214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 把训练结果保存下来，给后续训练使用\n",
    "output_path = Path(\"./output/03_mid_train_model.pt\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print(f\"模型已保存到 {output_path}\")"
   ],
   "id": "e1644cff1fd9ab68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存到 output/03_mid_train_model.pt\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
